//
//  Author: Hari Sekhon
//  Date: 2016-06-06 22:43:57 +0100 (Mon, 06 Jun 2016)
//
//  vim:ts=2:sts=2:sw=2:et
//
//  https://github.com/harisekhon/nagios-plugin-kafka
//
//  License: see accompanying Hari Sekhon LICENSE file
//
//  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish
//
//  https://www.linkedin.com/in/harisekhon
//

package com.linkedin.harisekhon.kafka

//import com.google.common.io.Resources

import java.io.File
import java.nio.file.Paths

import com.linkedin.harisekhon.CLI
import com.linkedin.harisekhon.Utils._
import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord}
import org.apache.kafka.clients.consumer.{ConsumerRecord, ConsumerRecords, KafkaConsumer}
import org.apache.kafka.common.TopicPartition
//import org.apache.kafka.common.protocol.SecurityProtocol.PLAINTEXT
//import org.apache.kafka.common.protocol.SecurityProtocol.SASL_PLAINTEXT
//import org.apache.kafka.common.protocol.SecurityProtocol.SASL_SSL

import scala.util.Random
//import java.io.InputStream
import java.util.Properties
import java.util.Arrays
import org.apache.log4j.Level
import org.apache.log4j.Logger
import collection.JavaConversions._
import java.text.SimpleDateFormat

object CheckKafka extends App {
    val check_kafka = new CheckKafka(
        broker_list = "192.168.99.100:9092",
        topic = "nagios-plugin-kafka-test",
        partition = 0,
        acks = "-1",
        // TODO: SASL_PLAINTEXT, SASL_SSL protocol testing
//        security_protocol = "SASL_PLAINTEXT",
        security_protocol = "PLAINTEXT",
        jaas_config = Option(null)
    )
    check_kafka.run()
}

class CheckKafka(
                        val broker_list: String = "localhost:9092",
                            val topic: String = "test",
                        val partition: Int = 0,
                        // ensure all ISRs have written msg
                        val acks: String = "-1",
                        val security_protocol: String = "PLAINTEXT",
                        var jaas_config: Option[String] = None
                ) {

    val log = Logger.getLogger("CheckKafka")
    // set in log4j.properties now
//    log.setLevel(Level.DEBUG)

    var jar = new File(classOf[CheckKafka].getProtectionDomain().getCodeSource().getLocation().toURI().getPath());
    if(jar.toString.contains("/target/")){
       jar = jar.getParentFile().getParentFile()
    }
    val jaas_default_config = Paths.get(jar.getParentFile().getAbsolutePath(), "kafka_cli_jaas.conf").toString;
    val jaas_prop = System.getProperty("java.security.auth.login.config")
    if(! jaas_config.isEmpty) {
        log.info(s"using JAAS config file arg '$jaas_config'")
    } else if (jaas_prop != null) {
        val jaas_file = new File(jaas_prop)
        if (jaas_file.exists() && jaas_file.isFile()) {
            jaas_config = Option(jaas_prop)
            log.info(s"using JAAS config file from System property java.security.auth.login.config = '$jaas_config'")
        } else {
            log.warn(s"JAAS path specified in System property java.security.auth.login.config = '$jaas_prop' does not exist!")
        }
    }
    if(jaas_config.isEmpty){
        val jaas_default_file = new File(jaas_default_config)
        if(jaas_default_file.exists() && jaas_default_file.isFile()){
            log.info(s"using default JaaS config file '$jaas_default_config'")
            jaas_config = Option(jaas_default_config)
        } else {
            log.warn("cannot find default JAAS file and none supplied")
        }
    }
    if(jaas_config.isDefined) {
        System.setProperty("java.security.auth.login.config", jaas_config.get)
    } else {
        log.warn("no JAAS config defined")
    }

    val uuid = java.util.UUID.randomUUID.toString
    val epoch = System.currentTimeMillis()
    // comes out the same whether specifying single, double or triple data digits
    val date = new SimpleDateFormat("yyyy-dd-MM HH:MM:ss.SSS Z").format(epoch)
    val id: String = s"Hari Sekhon check_kafka (scala) - random token=$uuid, $date"

    // enforce random group id
//    if(props.getProperty("group.id") == null) {
//        log.debug(s"group.id not set, creating random group id")
//    }

    val msg = s"test message generated by $id"
    log.debug(s"test message => '$msg'")

    val topic_partition = new TopicPartition(topic, partition)
    var last_offset: Long = 0

    val consumer_props = new Properties
    // old
//    consumer_props.put("metadata.broker.list", broker_list)
    consumer_props.put("bootstrap.servers", broker_list)
    consumer_props.put("security.protocol", security_protocol)

    // works without this
    val group_id: String = s"$uuid, $date"
    log.debug(s"group id='$group_id'")
//    consumer_props.put("group.id", group_id)

    consumer_props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
    consumer_props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")

    // trying to fail fast
//    props.put("timeout.ms", "5000") // 5 secs for ISR acks
//    props.put("metadata.fetch.timeout.ms", "1000") // 1 sec for metadata on topic connect
//    props.put("consumer.timeout.ms", "1000") // msg must be available within this window
//    props.put("socket.timeout.ms", "1000")
//    props.put("request.timeout.ms", "1000")
//    props.put("reconnect.backoff.ms", "0")
//    props.put("retry.backoff.ms", "0")
//    props.put("session.timeout.ms", "900")
//    props.put("fetch.max.wait.ms", "900")
//    props.put("heartbeat.interval.ms", "100")

    log.debug("creating Kafka consumer")
    // XXX: TODO: try switch to SimpleConsumer to be able to get rejected properly, also better to be backwards compatible with 0.8 systems
    val consumer = new KafkaConsumer[String, String](consumer_props)
//    var consumer: KafkaConsumer[String, String] //= new KafkaConsumer[String, String](props)

    val producer_props = new Properties
    producer_props.put("bootstrap.servers", broker_list)
    producer_props.put("client.id", "CheckKafka")
//    producer_props put("request.required.acks", required_acks)
    producer_props put("acks", acks)
    producer_props.put("security.protocol", security_protocol)
    producer_props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer")
    producer_props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer")

    log.debug("creating Kafka producer")
    //    val producer: KafkaProducer[String, String] = new KafkaProducer[String, String](props)
    val producer = new KafkaProducer[String, String](producer_props)

    def run(): Unit = {
        val start_time = System.currentTimeMillis()
        subscribe(topic)
        val start_write = System.currentTimeMillis()
        produce(topic, msg)
        // if clock gets reset and this become negative I'm not handling it as that should be a super rare one time occurrence
        // unless perhaps there are a lot of NTPd time steps to bring time back inline, but anyway that shouldn't be
        // a regular occurrence that affects this program
        val write_time = (System.currentTimeMillis() - start_write) / 1000.0
        val read_start_time = System.currentTimeMillis()
        consume(topic)
        val end_time = System.currentTimeMillis()
        val read_time = (end_time - read_start_time) / 1000.0
        val total_time = (end_time - start_time) / 1000.0
        val plural = if (broker_list.split("\\s+,\\s+").length > 1) "s" else ""
        println(s"OK: Kafka broker${plural} successfully returned unique message, write_time=${write_time}s, read_time=${read_time}s, total_time=${total_time}s | write_time=${write_time}s, read_time=${read_time}s, total_time=${total_time}s")
    }

    def subscribe(topic: String = topic): Unit = {
        // conflicts with partition assignment
//        log.debug(s"subscribing to topic $topic")
//        consumer.subscribe(Arrays.asList(topic))
        log.debug(s"consumer assigning topic '$topic' partition '$partition'")
        consumer.assign(Arrays.asList(topic_partition))
//        consumer.assign(Arrays.asList(partition))
        // not connected to port so no conn refused at this point
        // loops from here indefinitely if connection refused
        last_offset = consumer.position(topic_partition)
    }

    def produce(topic: String = topic, msg: String = msg): Unit = {
        //        InputStream props = Resources.getResource("file.properties").openStream()
//        try{
        log.debug(s"sending message to topic $topic partition $partition")
        producer.send(new ProducerRecord[String, String](topic, partition, id, msg)) // key and partition optional
        log.debug("flushing")
        producer.flush()
        log.debug("closing producer")
        producer.close() // blocks until msgs are sent
//        } catch(Throwable t){
//            println("%s", t.getStackTrace)
//        }
//        finally {
//            producer.close() // blocks until msgs are sent
//        }
    }

    def consume(topic: String = topic): Unit = {
        log.debug(s"seeking to last known offset $last_offset")
        consumer.seek(topic_partition, last_offset)
        log.debug(s"consuming from offset $last_offset")
        val records: ConsumerRecords[String, String] = consumer.poll(200) // ms
        log.debug("closing consumer")
        consumer.close()
        val consumed_record_count: Int = records.count()
        log.debug(s"consumed record count = $consumed_record_count")
        assert(consumed_record_count != 0)
        var msg2: String = null
        for (record: ConsumerRecord[String, String] <- records) {
            val record_topic = record.topic()
            val value = record.value()
            log.debug(s"found message, topic '$record_topic', value = '$value'")
            assert(topic.equals(record_topic))
            if (msg.equals(value)) {
                msg2 = value
            }
        }
        log.debug(s"message returned: $msg2")
        log.debug(s"message expected: $msg")
        if (msg2 == null) {
            println("CRITICAL: message not returned by Kafka")
            System.exit(2)
        } else if (!msg.equals(msg2)) {
            println("CRITICAL: message returned does not equal message sent!")
            System.exit(2)
        }
    }

}
